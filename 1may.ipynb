{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfeBKyBVeVIvTTSo4QY4r+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/golu628/assignment/blob/main/1may.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw4JbZ9ErUuL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Contingency Matrix and Classification Model Evaluation\n",
        "\n",
        "A contingency matrix, also known as a confusion matrix, is a table that visually represents the performance of a classification model. It shows the number of correct and incorrect predictions for each class. Here's how it's used:\n",
        "\n",
        "Rows represent the actual class labels.\n",
        "Columns represent the predicted class labels.\n",
        "The diagonal entries (where row and column match) represent correct predictions for each class.\n",
        "Off-diagonal entries represent incorrect predictions, where the model predicted a different class than the actual one.\n",
        "By analyzing the confusion matrix, you can calculate metrics like accuracy, precision, recall, and F1-score to assess the model's performance on different classes.\n",
        "\n",
        "Q2. Pair Confusion Matrix vs. Regular Confusion Matrix\n",
        "\n",
        "A regular confusion matrix focuses on individual classes. A pair confusion matrix, however, is used for multi-class classification tasks where the model predicts relationships between classes. It shows how often the model correctly predicts both classes in a pair (true positive) or makes mistakes (false positive, false negative, true negative).\n",
        "\n",
        "This is useful when the focus is on identifying specific relationships between classes, especially when incorrect predictions in these relationships have a higher cost.\n",
        "\n",
        "Q3. Extrinsic Measures in NLP\n",
        "\n",
        "Extrinsic measures evaluate the performance of a language model by integrating it into a downstream NLP task, such as machine translation, question answering, or sentiment analysis. They assess how well the model's output improves the overall performance of the application.\n",
        "\n",
        "In simpler terms, extrinsic measures test how well the language model performs in real-world use cases.\n",
        "\n",
        "Q4. Intrinsic vs. Extrinsic Measures\n",
        "\n",
        "Intrinsic measures evaluate the properties of the model itself, independent of any specific application. They focus on aspects like accuracy, fluency, or coherence in the model's output (e.g., BLEU score for machine translation).\n",
        "Extrinsic measures, as mentioned above, assess the model's impact on the performance of a downstream NLP task.\n",
        "Q5. Purpose and Use of Confusion Matrix\n",
        "\n",
        "A confusion matrix helps identify the strengths and weaknesses of a classification model by:\n",
        "\n",
        "Visualizing Prediction Errors: You can see which classes the model confuses most often (off-diagonal entries).\n",
        "Calculating Performance Metrics: Confusion matrix forms the basis for calculating metrics like precision, recall, and F1-score, which provide insights into specific aspects of the model's performance.\n",
        "Q6. Intrinsic Measures for Unsupervised Learning\n",
        "\n",
        "Here are some common intrinsic measures for unsupervised learning and their interpretations:\n",
        "\n",
        "Silhouette Coefficient: Measures how well data points are clustered within their assigned clusters compared to neighboring clusters. Higher values indicate better cluster separation.\n",
        "Calinski-Harabasz Index: Similar to the silhouette coefficient, it compares the within-cluster variance to the between-cluster variance. Higher values indicate better separation.\n",
        "Davies-Bouldin Index: Evaluates the ratio of the within-cluster scatter to the distance between cluster centers. Lower values indicate better clustering.\n",
        "Q7. Limitations of Accuracy and Addressing Them\n",
        "\n",
        "Accuracy, the percentage of correct predictions, is a simple metric but has limitations:\n",
        "\n",
        "Class Imbalance: If one class has significantly more data points, the model can achieve high accuracy by simply predicting the majority class most of the time.\n",
        "Focus on Overall Performance: Accuracy doesn't reveal how the model performs on specific classes."
      ],
      "metadata": {
        "id": "6IwGa4Z8rZPZ"
      }
    }
  ]
}