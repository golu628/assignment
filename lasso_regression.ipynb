{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMalkpw6u+WeNxu/Cwsk9wr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/golu628/assignment/blob/main/lasso_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cz2rjxcXvrQc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. R-squared (R²) Explained\n",
        "\n",
        "Concept: R-squared is a statistical measure used in linear regression to assess how well the regression line fits the data points. It represents the proportion of the variance in the dependent variable (Y) that can be explained by the independent variable(s) (X).\n",
        "\n",
        "Calculation: R-squared is calculated as 1 minus the ratio of the variance of the residuals (unexplained errors) to the total variance of the dependent variable.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "A value closer to 1 indicates a better fit, meaning the model explains a higher proportion of the variance in Y.\n",
        "A value of 0 means the model explains none of the variance, and a negative value (rarely occurs) indicates a worse fit than predicting the mean of Y.\n",
        "Q2. Adjusted R-squared (Adjusted R²)\n",
        "\n",
        "Definition: Adjusted R-squared is a modification of R-squared that penalizes for the number of features (independent variables) in the model. It accounts for the model's complexity and helps to compare models with different numbers of features.\n",
        "\n",
        "Difference from R-squared: R-squared can increase simply by adding more features, even if they are not truly explanatory. Adjusted R-squared addresses this by adjusting the R-squared value based on the sample size and number of features.\n",
        "\n",
        "Q3. When to Use Adjusted R-squared\n",
        "\n",
        "Adjusted R-squared is more appropriate when comparing models with different numbers of features. It helps to identify the model that explains the data better while considering its complexity. For models with the same number of features, regular R-squared can be sufficient.\n",
        "\n",
        "Q4. Error Metrics: RMSE, MSE, MAE\n",
        "\n",
        "These metrics quantify the difference between the predicted values (Yhat) from the model and the actual values (Y) in the data.\n",
        "\n",
        "Root Mean Squared Error (RMSE):\n",
        "\n",
        "Calculation: Square the residuals (Y - Yhat) for each data point, calculate the average of those squared values, and then take the square root.\n",
        "Represents: The average magnitude of the errors, considering both large and small errors.\n",
        "Mean Squared Error (MSE):\n",
        "\n",
        "Calculation: Similar to RMSE, but without taking the square root of the final average.\n",
        "Represents: The average squared difference between predicted and actual values. Easier to interpret for calculations involving variances.\n",
        "Mean Absolute Error (MAE):\n",
        "\n",
        "Calculation: Take the absolute difference between predicted and actual values for each data point, and then calculate the average.\n",
        "Represents: The average magnitude of the errors, focusing on the absolute differences rather than squares. Less sensitive to outliers than RMSE or MSE.\n",
        "Q5. Advantages and Disadvantages of Error Metrics\n",
        "\n",
        "RMSE:\n",
        "\n",
        "Advantages: Considers both large and small errors, providing a good overall measure of fit.\n",
        "Disadvantages: Sensitive to outliers, as squaring large errors amplifies their impact.\n",
        "MSE:\n",
        "\n",
        "Advantages: Easier to interpret in variance-based calculations.\n",
        "Disadvantages: Similar disadvantages to RMSE regarding outlier sensitivity.\n",
        "MAE:\n",
        "\n",
        "Advantages: Less sensitive to outliers than RMSE or MSE, good for capturing the average magnitude of errors.\n",
        "Disadvantages: Ignores the direction of errors (positive or negative), doesn't provide information on the spread of errors.\n",
        "Choosing the Right Metric:\n",
        "\n",
        "The best metric depends on your specific needs. If outliers are a concern, use MAE. If understanding the spread of errors is important, consider RMSE. MSE is often used for mathematical convenience in calculations.\n",
        "\n",
        "Q6. Lasso vs. Ridge Regularization\n",
        "\n",
        "These techniques address overfitting in linear regression by penalizing the model's complexity.\n",
        "\n",
        "Lasso Regularization:\n",
        "\n",
        "Adds a penalty term to the cost function that is the sum of the absolute values of the regression coefficients.\n",
        "Shrinks some coefficients to zero, effectively removing those features from the model.\n",
        "Useful for feature selection when some features might be irrelevant or redundant.\n",
        "Ridge Regularization:\n",
        "\n",
        "Adds a penalty term to the cost function that is the sum of the squared values of the regression coefficients.\n",
        "Shrinks all coefficients towards zero, but not necessarily to zero.\n",
        "Useful when all features might be relevant but contribute to model complexity.\n",
        "Q7. Continued: Regularization to Prevent Overfitting (Example)\n",
        "\n",
        "Let's continue the example from Q7:\n",
        "\n",
        "Imagine a model fitting a random squiggly line through all the data points in the training set. This might have a high R-squared on the training data, but when you use it to predict for new unseen data points, it will likely perform poorly because it's too focused on fitting the noise in the training data rather than the underlying trend.\n",
        "\n",
        "Regularization techniques like Lasso or Ridge penalize the model for having too many complex features or large coefficients. This discourages the model from fitting the random noise and encourages it to capture the general trend, leading to better generalization on unseen data.\n",
        "\n",
        "Q8. Limitations of Regularized Linear Models\n",
        "\n",
        "While regularized models help prevent overfitting, they have limitations:\n",
        "\n",
        "Bias-Variance Trade-off: Regularization reduces variance (overfitting) but can introduce bias. By shrinking coefficients, the model might underfit the data, leading to systematic errors in predictions. Finding the right balance between reducing variance and avoiding bias is crucial.\n",
        "Feature Selection Issues: Lasso performs feature selection by setting some coefficients to zero. This can be helpful, but it might also remove relevant features if they are weakly correlated with the target variable.\n",
        "Not a Cure-All: Regularization doesn't guarantee perfect model performance. Choosing the right features and model complexity is still important.\n",
        "Q9. Choosing Between RMSE and MAE\n",
        "\n",
        "Here's how you might decide between RMSE and MAE in the scenario you described:\n",
        "\n",
        "Model A with RMSE of 10: This indicates that on average, the errors between predicted and actual values are around 10 units in your chosen scale. However, RMSE is sensitive to outliers.\n",
        "\n",
        "Model B with MAE of 8: This suggests that the average absolute difference between predicted and actual values is 8 units. This is less affected by outliers.\n",
        "\n",
        "Choosing the Model:\n",
        "\n",
        "Without additional information, it's difficult to definitively say which model is better. Here's why:\n",
        "\n",
        "If outliers are a concern: Choose Model B (MAE) as it's less sensitive to them.\n",
        "If understanding the spread of errors is important: Consider Model A (RMSE) as it captures both large and small errors.\n",
        "Limitations:\n",
        "\n",
        "Both metrics only provide a single measure of error. Visualizing the distribution of errors (e.g., histograms) can provide more insights.\n",
        "The choice of metric depends on your specific application and what aspect of error is most relevant.\n",
        "Q10. Choosing Between Regularization Techniques\n",
        "\n",
        "Selecting between Ridge and Lasso regularization depends on your data and goals:\n",
        "\n",
        "Model A (Ridge) with regularization parameter 0.1: Ridge shrinks all coefficients towards zero but doesn't necessarily eliminate any features.\n",
        "\n",
        "Model B (Lasso) with regularization parameter 0.5: Lasso might set some coefficients to zero, effectively removing those features from the model.\n",
        "\n",
        "Choosing the Model:\n",
        "\n",
        "If feature selection is important: Consider Model B (Lasso) if you suspect some features are irrelevant or redundant.\n",
        "If you want to retain all features but reduce their impact: Model A (Ridge) might be preferable.\n",
        "Trade-offs and Limitations:\n",
        "\n",
        "The choice of regularization parameter (0.1 or 0.5 in your example) also plays a role. A higher value leads to stronger regularization and potentially more bias.\n",
        "There's no one-size-fits-all solution. Experiment with different regularization techniques and parameters to see what works best for your data and model"
      ],
      "metadata": {
        "id": "GqZ-MBa7vtfc"
      }
    }
  ]
}