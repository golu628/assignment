{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiFdZeKN+1dgxM6mJBmYRt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/golu628/assignment/blob/main/18_feb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHYAet_MzyRQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Filter Methods\n",
        "\n",
        "Concept: Filter methods employ statistical measures to evaluate features independently of a specific machine learning model. They assess the features' intrinsic properties, such as relevance to the target variable or redundancy within the feature set.\n",
        "How it works:\n",
        "Calculate a score for each feature based on a chosen metric (e.g., information gain, chi-square, correlation coefficient for numerical features).\n",
        "Rank features based on their scores.\n",
        "Select a predefined number of top-ranked features or apply a threshold to filter out features below a certain score.\n",
        "2. Wrapper Methods\n",
        "\n",
        "Concept: Wrapper methods use a machine learning model as a scoring mechanism to evaluate the effectiveness of different feature subsets. They iteratively select/remove features and assess the impact on model performance on a validation set.\n",
        "How it works:\n",
        "Define a search strategy (e.g., forward selection, backward selection, exhaustive search).\n",
        "Train the model with different combinations of features based on the search strategy.\n",
        "Evaluate the model's performance on a validation set for each feature combination.\n",
        "Select the feature subset that leads to the best model performance.\n",
        "3. Embedded Methods\n",
        "\n",
        "Concept: Embedded methods integrate feature selection within the model training process. These methods penalize model complexity, which often leads to selecting only the most important features implicitly.\n",
        "Common techniques:\n",
        "L1 Regularization (LASSO): Shrinks feature coefficients towards zero, potentially eliminating irrelevant features with coefficients driven to zero.\n",
        "Tree-based methods (Random Forest, Gradient Boosting): Assign feature importance scores based on their contribution to splitting decisions in the trees.\n",
        "Choosing a Feature Selection Method\n",
        "\n",
        "The choice between Filter, Wrapper, and Embedded methods depends on several factors:\n",
        "\n",
        "Dataset size: Filter methods are generally faster, making them suitable for large datasets. Wrapper methods can be computationally expensive on large datasets.\n",
        "Model complexity: Wrapper methods can outperform Filter methods for complex models, but at a computational cost.\n",
        "Interpretability: Filter methods may provide insights into feature relevance, while Wrapper and Embedded methods might be less interpretable.\n",
        "Applications in Specific Scenarios\n",
        "\n",
        "Q6. Telecom Customer Churn Prediction (Filter Method)\n",
        "\n",
        "Identify relevant features: Consider features like customer demographics, service usage patterns, contract details, and billing history.\n",
        "Choose a Filter method: Information gain, chi-square, or correlation coefficient could be suitable options.\n",
        "Calculate feature scores: Evaluate each feature's relationship with the target variable (customer churn).\n",
        "Select features: Choose a subset of top-ranked features based on their scores or apply a threshold.\n",
        "Q7. Soccer Match Outcome Prediction (Embedded Method)\n",
        "\n",
        "Feature selection using LASSO regularization: Include player statistics (goals, assists, saves), team rankings, historical performance data, and potentially external factors (weather).\n",
        "Train a model (e.g., logistic regression): LASSO will automatically penalize coefficients, potentially driving coefficients of less important features to zero, effectively selecting features during training.\n",
        "Evaluate feature importance: Analyze the model coefficients post-training to identify the most influential features for prediction.\n",
        "Q8. House Price Prediction (Wrapper Method)\n",
        "\n",
        "Define features: Include size, location (categorical or numerical based on encoding), number of bedrooms, bathrooms, age, and amenities.\n",
        "Choose a search strategy: Forward selection is a common approach.\n",
        "Evaluate feature subsets: Train the model (e.g., linear regression) with different combinations of features based on the search strategy.\n",
        "Select the optimal subset: Choose the feature combination that yields the best performance on a validation set."
      ],
      "metadata": {
        "id": "nn3dldzZz3sq"
      }
    }
  ]
}