{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEWheQI5KgnH/TWjcByzQ6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/golu628/assignment/blob/main/linear_regression_1st.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6Kt7Htfu3N8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Simple vs. Multiple Linear Regression\n",
        "\n",
        "Simple Linear Regression: This method models the relationship between one independent variable (X) and a dependent variable (Y). It essentially finds a straight line that best fits the data points, aiming to predict Y based on changes in X.\n",
        "\n",
        "Example: Predicting house price (Y) based on its square footage (X).\n",
        "Multiple Linear Regression: This is an extension that models the relationship between a dependent variable (Y) and two or more independent variables (X1, X2, ..., Xn). It finds a hyperplane (multidimensional plane) that best fits the data in higher dimensions, allowing for the influence of multiple factors on Y.\n",
        "\n",
        "Example: Predicting car price (Y) based on mileage (X1), model year (X2), and engine size (X3).\n",
        "Q2. Assumptions of Linear Regression and Checking Them\n",
        "Linear regression relies on several assumptions for accurate results:\n",
        "\n",
        "Linearity: The relationship between X and Y is linear (straight line).\n",
        "Homoscedasticity: The variance of the errors (difference between predicted and actual Y) is constant across all X values.\n",
        "Independence: The errors are independent of each other (no correlation).\n",
        "Normality: The errors are normally distributed.\n",
        "These assumptions can be checked through visual inspection of plots (e.g., scatter plots with fitted line) and statistical tests. Software tools often provide diagnostics for these checks.\n",
        "\n",
        "Q3. Interpreting Slope and Intercept\n",
        "Slope (coefficient of X): This value indicates how much Y changes on average for a one-unit increase in X, holding all other independent variables constant.\n",
        "\n",
        "Example: In the house price example, a slope of 100 might mean that for every additional square foot, the predicted house price increases by $100 (assuming a positive slope).\n",
        "Intercept (Y-intercept): This is the predicted value of Y when all independent variables (X) are zero (if applicable in the context). It represents the starting point of the regression line on the Y-axis.\n",
        "\n",
        "Remember: The interpretation considers the specific context and units of your variables.\n",
        "\n",
        "Q4. Gradient Descent Explained\n",
        "Gradient Descent: An optimization algorithm commonly used in machine learning to minimize the cost function (measure of error) in a model. It iteratively adjusts the model's parameters (like slopes and intercepts in linear regression) in the direction that leads to the steepest decrease in the cost function, ultimately leading to a better fit for the data.\n",
        "\n",
        "Use in Machine Learning: Gradient descent is a fundamental concept for training various machine learning models, including linear regression, neural networks, and logistic regression. It helps the model learn from the data by adjusting its internal parameters to achieve the best possible performance on the given task.\n",
        "\n",
        "Understanding Multiple Linear Regression (MLR)\n",
        "Q5. Multiple Linear Regression (MLR) vs. Simple Linear Regression\n",
        "\n",
        "MLR builds upon simple linear regression by incorporating multiple independent variables to explain the dependent variable. It's like fitting a hyperplane in higher dimensions to capture the complex relationships between multiple factors and the outcome.\n",
        "\n",
        "Key Differences:\n",
        "\n",
        "Number of Independent Variables: Simple linear regression uses one, while MLR uses two or more.\n",
        "Model Complexity: MLR models are generally more complex due to the additional variables and their potential interactions.\n",
        "Q6. Multicollinearity in MLR and How to Address It\n",
        "\n",
        "Multicollinearity: This occurs when two or more independent variables in MLR are highly correlated with each other. It can cause problems in the model, making it difficult to interpret the individual effects of each variable.\n",
        "\n",
        "Detection: High correlation coefficients (> 0.8 or 0.9) between independent variables can indicate multicollinearity.\n",
        "\n",
        "Addressing Multicollinearity:\n",
        "\n",
        "Remove highly correlated variables: If one variable can be explained by another, removing it might be appropriate.\n",
        "Dimensionality reduction techniques: Use techniques like Principal Component Analysis (PCA) to reduce the number of variables while preserving most of the information.\n",
        "Polynomial Regression: Fitting Curves\n",
        "Q7. Polynomial Regression vs. Linear Regression\n",
        "\n",
        "Polynomial Regression: This method fits a curved line (polynomial) to the data points instead of a straight line in linear regression. It allows for modeling non-linear relationships between X and Y.\n",
        "\n",
        "Differences:\n",
        "\n",
        "Model Complexity: Polynomial regression models are more complex due to the use of higher-order terms (X^2, X^3, etc.).\n",
        "Flexibility: Polynomial regression can capture non-linear patterns that linear regression cannot.\n",
        "Q8. Advantages and Disadvantages\n",
        "Advantages and Disadvantages of Polynomial Regression vs. Linear Regression\n",
        "Advantages of Polynomial Regression:\n",
        "\n",
        "Flexibility: Polynomial regression excels at capturing non-linear relationships between the independent and dependent variables. When data exhibits curves, trends, or peaks, polynomial regression can provide a more accurate fit than the straight line of linear regression.\n",
        "\n",
        "Improved Accuracy: In situations where the underlying relationship is truly non-linear, polynomial regression can significantly improve the model's accuracy compared to forcing a linear fit. This can lead to better predictions and more reliable insights.\n",
        "\n",
        "Versatility: Polynomial regression can model a wide range of non-linear patterns by adjusting the degree of the polynomial (the highest power of X used). This allows for flexibility in adapting the model to different types of non-linear relationships.\n",
        "\n",
        "Disadvantages of Polynomial Regression:\n",
        "\n",
        "Overfitting: A major drawback of polynomial regression is its susceptibility to overfitting. With increased model complexity (higher polynomial degrees), the model can start fitting random noise in the data  rather than the actual underlying trend. This leads to a model that performs well on the training data but poorly on unseen data.\n",
        "\n",
        "Loss of Interpretability: As the polynomial degree increases, the model becomes more complex and the interpretation of coefficients becomes less straightforward. It can be difficult to understand the individual impact of each variable on the outcome.\n",
        "\n",
        "Higher Variance: Polynomial regression models tend to have higher variance compared to linear regression. This means that small changes in the data can lead to significant changes in the fitted curve. This can make the model less reliable for prediction on new data points.\n",
        "\n",
        "Choosing Between Linear and Polynomial Regression:\n",
        "\n",
        "The choice between linear and polynomial regression depends on the nature of your data and the problem you're trying to solve. Here's a general guideline:\n",
        "\n",
        "Use linear regression: If the relationship between X and Y appears to be linear (straight line), or if interpretability of the model is a priority, then linear regression is a good choice. It's simpler, less prone to overfitting, and easier to interpret.\n",
        "Use polynomial regression: If the data clearly exhibits a non-linear pattern (curves, trends, peaks), and capturing this non-linearity is crucial for your analysis, then polynomial regression can be a better option. However, be cautious of overfitting and ensure you have enough data to support the increased model complexity.\n",
        "Additional Considerations:\n",
        "\n",
        "Data Visualization: Always start with visualizing your data to understand the underlying relationship between X and Y. This will help you identify if a linear or non-linear model is more appropriate.\n",
        "Model Selection Techniques: Techniques like cross-validation can be used to compare the performance of linear and polynomial regression models on unseen data. This helps you choose the model that generalizes better and avoids overfitting.\n",
        "Regularization: Regularization techniques can be applied to polynomial regression to reduce overfitting by penalizing overly complex models."
      ],
      "metadata": {
        "id": "BIT_H7k2u6QH"
      }
    }
  ]
}