{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPo5RWpAV7aM+YBQ/U4CnSj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/golu628/assignment/blob/main/29_march.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Lasso Regression and its Differences\n",
        "\n",
        "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a statistical technique used in linear regression for both prediction and feature selection. It differs from other regression techniques in two key aspects:\n",
        "\n",
        "Regularization: Like Ridge Regression, Lasso adds a penalty term to the objective function. However, unlike Ridge which uses the L2 norm (sum of squared coefficients), Lasso uses the L1 norm (sum of absolute values of coefficients).\n",
        "Sparsity: This L1 penalty in Lasso encourages sparsity, meaning it drives some coefficient values to exactly zero. This effectively performs feature selection by removing irrelevant features from the model.\n",
        "Q2. Lasso Regression for Feature Selection\n",
        "\n",
        "The main advantage of Lasso Regression is its ability to perform automatic feature selection. By shrinking coefficients to zero, Lasso identifies features that contribute minimally and removes them from the final model. This leads to a simpler, more interpretable model and potentially reduces overfitting.\n",
        "\n",
        "Q3. Interpreting Coefficients in Lasso Regression\n",
        "\n",
        "Interpreting coefficients in Lasso is trickier compared to OLS regression. Due to shrinkage, some coefficients might be zero, making it difficult to assess individual feature contributions. Here are some approaches:\n",
        "\n",
        "Focus on non-zero coefficients: Analyze the features with non-zero coefficients as these are likely the most important ones for prediction.\n",
        "Feature importance techniques: Use techniques designed for regularized models, like permutation importance or L1 path analysis, to understand feature importance.\n",
        "Q4. Tuning Parameters in Lasso Regression\n",
        "\n",
        "The main tuning parameter in Lasso Regression is the regularization parameter (lambda). Here's how it affects the model:\n",
        "\n",
        "Higher lambda: Shrinks coefficients more aggressively, leading to a sparser model with fewer features but potentially higher bias (underfitting).\n",
        "Lower lambda: Relaxes the penalty, resulting in a more complex model with potentially lower bias but higher variance (overfitting).\n",
        "Q5. Lasso Regression for Non-linear Problems\n",
        "\n",
        "Lasso Regression is primarily designed for linear models. However, it can be used in non-linear settings through feature engineering. You can create new features by transforming existing features (e.g., polynomial terms, interactions) to capture non-linear relationships. Lasso will then perform selection among these transformed features.\n",
        "\n",
        "Q6. Lasso vs. Ridge Regression\n",
        "\n",
        "Both Ridge and Lasso are regularization techniques, but they differ in their penalty terms:\n",
        "\n",
        "Ridge: L2 penalty shrinks coefficients towards zero but doesn't set them to zero, leading to all features remaining in the model.\n",
        "Lasso: L1 penalty drives coefficients to zero, performing feature selection and creating a sparser model.\n",
        "Q7. Lasso Regression and Multicollinearity\n",
        "\n",
        "Lasso Regression can handle multicollinearity effectively. The L1 penalty encourages sparsity, and when features are highly correlated, Lasso tends to select only one or a few from the group, reducing the influence of multicollinearity on the model.\n",
        "\n",
        "Q8. Choosing the Optimal Lambda\n",
        "\n",
        "Finding the optimal lambda value is crucial for Lasso performance. Here are common methods:\n",
        "\n",
        "Cross-validation: Similar to Ridge Regression, split data into training and validation sets. Train models with different lambdas and choose the one with the best performance on the validation set.\n",
        "Information Criteria: Use metrics like AIC or BIC that consider model complexity and performance. The lambda with the lowest score is preferred."
      ],
      "metadata": {
        "id": "7NroNyedGAMF"
      }
    }
  ]
}