{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvfsZ7QqNoM6hnT3/jmp+D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/golu628/assignment/blob/main/knn_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1WDhktpXdYO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is the KNN algorithm?\n",
        "\n",
        "The K-Nearest Neighbors (KNN) algorithm is a fundamental supervised machine learning method used for both classification and regression tasks. It works by:\n",
        "\n",
        "Storing the training data: KNN doesn't explicitly learn a model during training. Instead, it retains the entire training dataset as a reference.\n",
        "\n",
        "Calculating distances: When presented with a new data point (query point), KNN calculates the distance between the query point and all points in the training data using a distance metric like Euclidean distance or Manhattan distance (more on these in Q9).\n",
        "\n",
        "Finding nearest neighbors: Based on the chosen distance metric, KNN identifies the k closest data points (neighbors) to the query point. The value of k is a hyperparameter that you need to tune.\n",
        "\n",
        "Making predictions:\n",
        "\n",
        "Classification: For classification problems, KNN predicts the class label (e.g., spam/not spam, cat/dog) that appears most frequently among the k nearest neighbors. This is called majority voting.\n",
        "Regression: For regression problems, KNN predicts the continuous value (e.g., house price, temperature) by averaging the values of the k nearest neighbors.\n",
        "KNN essentially relies on the principle that data points close together in the feature space are likely to belong to the same class (classification) or have similar values (regression).\n",
        "\n",
        "Q2. How do you choose the value of K in KNN?\n",
        "\n",
        "Choosing the optimal value of k is crucial for KNN's performance. Here are some guidelines:\n",
        "\n",
        "Experimentation: There's no one-size-fits-all answer. Try different values of k using techniques like cross-validation and evaluate the model's performance on a held-out validation set.\n",
        "Bias-variance trade-off: Lower k values (fewer neighbors) can lead to high variance (overfitting) but low bias. Higher k values (more neighbors) can result in high bias (underfitting) but lower variance.\n",
        "Data characteristics: Consider the complexity of your data. Noisy data or data with many outliers might benefit from higher k to reduce sensitivity to noise.\n",
        "Q3. What is the difference between KNN classifier and KNN regressor?\n",
        "\n",
        "The key difference lies in the prediction step:\n",
        "\n",
        "KNN classifier: Predicts the class label with the majority vote among the k nearest neighbors.\n",
        "KNN regressor: Predicts a continuous value by averaging the values of the k nearest neighbors.\n",
        "Both utilize the same distance-based approach but cater to different prediction types: classification (categories) vs. regression (continuous values).\n",
        "\n",
        "Q4. How do you measure the performance of KNN?\n",
        "\n",
        "Here are common metrics for evaluating KNN performance depending on the task type:\n",
        "\n",
        "Classification: Accuracy, precision, recall, F1-score, confusion matrix\n",
        "Regression: Mean squared error (MSE), R-squared\n",
        "Choose metrics that align with your specific problem and success criteria.\n",
        "\n",
        "Q5. What is the curse of dimensionality in KNN?\n",
        "\n",
        "The curse of dimensionality refers to the challenges that arise when dealing with high-dimensional data (many features). In KNN, this can lead to:\n",
        "\n",
        "Increased distance calculations: Distances calculated in high dimensions become less meaningful, making it harder to identify true nearest neighbors.\n",
        "Sparsity of data: Data points become scattered as dimensionality increases, making it difficult to find truly close neighbors, especially for small datasets.\n",
        "Q6. How do you handle missing values in KNN?\n",
        "\n",
        "Missing values can negatively impact KNN's performance. Here are some strategies:\n",
        "\n",
        "Imputation: Fill in missing values using techniques like mean/median imputation or more sophisticated methods like KNN imputation (filling with the most frequent value among k nearest neighbors in the training data).\n",
        "Distance metrics that handle missing values: Consider using distance metrics robust to missing values, such as similarity-based metrics or metrics that can accommodate missing features.\n",
        "Removing data points: If missing values are extensive or crucial for prediction, it might be necessary to remove data points with too many missing features.\n",
        "Q7. KNN Classifier vs. Regressor Performance\n",
        "\n",
        "KNN Classifier:\n",
        "\n",
        "Strengths: Simple to understand and implement, effective for multi-class classification, non-parametric (no assumptions about data distribution), robust to outliers in some cases.\n",
        "Weaknesses: Prone to the curse of dimensionality, computationally expensive for large datasets due to distance calculations, high memory usage for storing the entire training data.\n",
        "KNN Regressor:\n",
        "\n",
        "Strengths: Simple to understand and implement, works well for continuous target variables.\n",
        "Weaknesses: Shares the same weaknesses as the KNN"
      ],
      "metadata": {
        "id": "rA8i_-UgXfJj"
      }
    }
  ]
}