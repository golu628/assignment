{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMj8b1TZRM6bIn9AaupyZMG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/golu628/assignment/blob/main/knn2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qmzcQlJZGcM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Euclidean vs. Manhattan Distance Metrics in KNN\n",
        "\n",
        "Euclidean Distance: This common metric calculates the straight-line distance between two data points in n-dimensional space. It considers all feature contributions equally.\n",
        "\n",
        "Manhattan Distance: This metric calculates the total distance along each dimension, like traveling through city streets. It emphasizes differences in specific features.\n",
        "\n",
        "Performance Impact:\n",
        "\n",
        "Euclidean distance might be better for balanced data with spherical clusters.\n",
        "Manhattan distance can be more robust to outliers in certain directions.\n",
        "Q2. Choosing the Optimal Value of k\n",
        "\n",
        "There's no single \"best\" k value. Here are techniques to find an optimal k:\n",
        "\n",
        "Cross-Validation: Split your data into folds, train on k-NN with different k values on each fold, and evaluate performance (accuracy for classification, mean squared error for regression) on the hold-out fold. Choose the k that minimizes the average error across folds.\n",
        "Grid Search: Try a range of k values, evaluate performance using cross-validation, and choose the k with the best performance.\n",
        "Q3. Distance Metric Impact on Performance\n",
        "\n",
        "The choice of distance metric can significantly affect KNN performance:\n",
        "\n",
        "Data Distribution: Match the metric to your data's distribution. If clusters are spherical, Euclidean distance might be better. For elongated clusters, Manhattan distance might work well.\n",
        "Feature Scaling: If features have different scales, consider scaling them to avoid biasing the distance calculation towards features with larger scales.\n",
        "Choosing a Metric:\n",
        "\n",
        "For balanced, spherically distributed data, start with Euclidean distance.\n",
        "If outliers or specific feature importance are concerns, consider Manhattan distance.\n",
        "Experiment with both metrics and compare performance using cross-validation.\n",
        "Q4. Common KNN Hyperparameters\n",
        "\n",
        "k: The number of nearest neighbors. (Discussed in Q2)\n",
        "Distance Metric: Euclidean, Manhattan, or others (discussed in Q3).\n",
        "Weights: Assign weights to neighbors based on their distance (closer neighbors get higher weights).\n",
        "p-norm: Adjust the emphasis on outliers with different power values in the distance calculation (e.g., p=1 for Manhattan, p=2 for Euclidean).\n",
        "Hyperparameter Tuning:\n",
        "\n",
        "Use techniques like grid search or random search to explore different hyperparameter combinations.\n",
        "Evaluate performance using cross-validation.\n",
        "Choose the combination that minimizes the error metric.\n",
        "Q5. Training Set Size and Optimization\n",
        "\n",
        "The size of the training set significantly impacts KNN performance:\n",
        "\n",
        "Larger datasets lead to more accurate nearest neighbor searches.\n",
        "However, storing and processing a massive dataset can become computationally expensive.\n",
        "Optimization Techniques:\n",
        "\n",
        "K-D Trees: Efficient data structures for fast nearest neighbor search, especially in high dimensions.\n",
        "Locality-Sensitive Hashing (LSH): Approximate nearest neighbor search for very large datasets.\n",
        "Early Stopping: Stop training once the model's performance plateaus to avoid overfitting.\n",
        "Consider sampling: If computational resources are limited, sample a subset of the data for training and use techniques like bagging or boosting to improve performance.\n",
        "Q6. Potential Drawbacks of KNN and How to Address Them\n",
        "\n",
        "Curse of Dimensionality: KNN performance can suffer in high-dimensional spaces due to increased distance similarity between points.\n",
        "High Computational Cost: Finding nearest neighbors can be computationally expensive for large datasets (addressed by techniques in Q5).\n",
        "Sensitivity to Noise: Outliers can significantly impact KNN results (addressed by robust distance metrics or filtering).\n",
        "Overcoming Drawbacks:\n",
        "\n",
        "Feature Selection: Choose informative features that reduce dimensionality and improve performance.\n",
        "Dimensionality Reduction Techniques: Use techniques like Principal Component Analysis (PCA) to project data into a lower-dimensional space.\n",
        "Regularization: Techniques like L1 or L2 regularization can help reduce overfitting.\n",
        "By understanding these factors and applying appropriate techniques, you can effectively use KNN classifiers and regressors in various machine learning tasks.\n",
        "\n",
        "pen_spark\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tune\n",
        "\n",
        "share\n",
        "\n",
        "\n",
        "more_vert\n"
      ],
      "metadata": {
        "id": "fo61suHXZMyv"
      }
    }
  ]
}