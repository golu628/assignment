{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMzgZDA6frCWNIEcmUbc8Ax",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/golu628/assignment/blob/main/ensemble_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N90rSohDv1Mz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. How does bagging reduce overfitting in decision trees?\n",
        "\n",
        "Decision trees are prone to overfitting, especially when they become very deep and complex. Bagging helps reduce overfitting in decision trees in a few ways:\n",
        "\n",
        "Model Averaging: By averaging the predictions from multiple, slightly different trees (trained on bootstrapped samples), bagging reduces the impact of any single tree's idiosyncrasies and leads to a smoother, less overfitted model.\n",
        "Reduced Variance: Each tree in the ensemble learns from a different subset of the data. This reduces the reliance on any particular data points that might be causing the tree to overfit.\n",
        "Q2. Advantages and Disadvantages of Different Base Learners:\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Diversity: Using different base learners in bagging can be beneficial if they learn different patterns in the data. This diversity can lead to a more robust ensemble model.\n",
        "Flexibility: If you're unsure about the best model type for your problem, bagging with different base learners allows you to explore various options and potentially achieve better performance.\n",
        "Disadvantages:\n",
        "\n",
        "Increased Complexity: Ensembles with diverse base learners can be more complex to interpret and understand compared to ensembles with a single base learner type.\n",
        "Training Time: Training multiple models of different types can be computationally expensive compared to training an ensemble with a single base learner type.\n",
        "Q3. Bias-Variance Tradeoff in Bagging:\n",
        "\n",
        "The choice of base learner can affect the bias-variance tradeoff in bagging:\n",
        "\n",
        "Low Bias Learners: If you use base learners with low bias (e.g., linear regression), the ensemble might still have a high bias, potentially underfitting the data.\n",
        "High Bias Learners: Using base learners with high bias (e.g., very deep decision trees) can lead to a higher variance ensemble, susceptible to overfitting if not carefully controlled.\n",
        "Q4. Bagging for Classification and Regression:\n",
        "\n",
        "Yes, bagging can be used for both classification and regression tasks.\n",
        "\n",
        "Classification: In classification, the final prediction is typically the majority vote (for most frequent class) or the average probability (for probabilistic models) of the individual models' predictions.\n",
        "Regression: For regression, the final prediction is usually the average of the individual models' predictions.\n",
        "Q5. Ensemble Size in Bagging:\n",
        "\n",
        "The ensemble size (number of models) in bagging can impact performance:\n",
        "\n",
        "Too Small: A small ensemble might not capture enough diversity and might not significantly reduce variance compared to a single model.\n",
        "Too Large: A very large ensemble might lead to diminishing returns and increased computational cost.\n",
        "There's no single best size; it often depends on the dataset and problem complexity. In practice, you can start with a small ensemble size and gradually increase it while monitoring performance on a validation set.\n",
        "\n",
        "Q6. Real-World Application of Bagging:\n",
        "\n",
        "Here's an example of bagging in action:\n",
        "\n",
        "Spam Filtering: Email service providers can use bagging with decision trees to classify incoming emails as spam or not spam. By using an ensemble of diverse decision trees, the filter can improve its accuracy and avoid overfitting to specific spam patterns.\n",
        "Overall, bagging is a powerful technique for reducing overfitting and improving the performance of models like decision trees. Understanding its impact on bias-variance and choosing appropriate base learners can help you leverage its benefits effectively."
      ],
      "metadata": {
        "id": "QfUKR-r0v2BH"
      }
    }
  ]
}