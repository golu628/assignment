{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOXXagrnM3yquJF8KEQVvLs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/golu628/assignment/blob/main/boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io9-qzyNaOI8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Gradient Boosting Regression?\n",
        "Gradient Boosting Regression is an ensemble learning technique that builds a strong predictive model by combining multiple weak learners, typically decision trees. It builds models sequentially, where each model tries to correct the errors of its predecessor using the gradient of a loss function.\n",
        "\n",
        "Q2. Implement Gradient Boosting Regression from Scratch\n",
        "Here's a simple implementation using Python and NumPy:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Generate a simple dataset\n",
        "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "y = np.sin(X).ravel() + np.random.normal(scale=0.1, size=100)\n",
        "\n",
        "# Gradient Boosting from scratch\n",
        "class GradientBoostingRegressorScratch:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_depth = max_depth\n",
        "        self.models = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y_pred = np.zeros(len(y))\n",
        "        for i in range(self.n_estimators):\n",
        "            residual = y - y_pred\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            tree.fit(X, residual)\n",
        "            update = tree.predict(X)\n",
        "            y_pred += self.learning_rate * update\n",
        "            self.models.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.zeros(X.shape[0])\n",
        "        for tree in self.models:\n",
        "            y_pred += self.learning_rate * tree.predict(X)\n",
        "        return y_pred\n",
        "\n",
        "# Train model\n",
        "model = GradientBoostingRegressorScratch(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "# Evaluate\n",
        "print(\"MSE:\", mean_squared_error(y, y_pred))\n",
        "print(\"RÂ² Score:\", r2_score(y, y_pred))\n",
        "Q3. Experiment with Hyperparameters\n",
        "You can use Grid Search to try combinations of learning rates, estimators, and depths:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [2, 3, 4]\n",
        "}\n",
        "\n",
        "best_score = -np.inf\n",
        "best_params = None\n",
        "\n",
        "for params in ParameterGrid(param_grid):\n",
        "    model = GradientBoostingRegressorScratch(**params)\n",
        "    model.fit(X, y)\n",
        "    score = r2_score(y, model.predict(X))\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_params = params\n",
        "\n",
        "print(\"Best RÂ² Score:\", best_score)\n",
        "print(\"Best Parameters:\", best_params)\n",
        "Q4. What is a Weak Learner in Gradient Boosting?\n",
        "A weak learner is a model that performs slightly better than random guessing. In Gradient Boosting, decision trees with shallow depth (often 1â€“3) are used as weak learners. Each weak learner tries to correct the residuals (errors) of the combined model so far.\n",
        "\n",
        "Q5. What is the Intuition Behind Gradient Boosting?\n",
        "Think of it as gradient descent in function space:\n",
        "\n",
        "Start with a simple prediction (e.g., the mean).\n",
        "\n",
        "At each step, compute the residuals (what the current model is getting wrong).\n",
        "\n",
        "Fit a weak model to these residuals.\n",
        "\n",
        "Update the ensemble by adding the predictions of the new model, scaled by a learning rate.\n",
        "\n",
        "Each new model reduces the error gradually, similar to how gradient descent reduces the loss.\n",
        "\n",
        "Q6. How Does Gradient Boosting Build an Ensemble?\n",
        "Initialize the model with a base prediction (e.g., mean of targets).\n",
        "\n",
        "For each iteration:\n",
        "\n",
        "Compute residuals (negative gradients of loss function).\n",
        "\n",
        "Train a weak learner (e.g., decision tree) on the residuals.\n",
        "\n",
        "Update the model by adding this weak learner's predictions.\n",
        "\n",
        "Final prediction is the sum of all weak learnersâ€™ outputs scaled by the learning rate.\n",
        "\n",
        "Q7. Steps to Construct Mathematical Intuition of Gradient Boosting\n",
        "Define a loss function, e.g., Mean Squared Error.\n",
        "\n",
        "Initialize the model with a constant prediction that minimizes the loss.\n",
        "\n",
        "For each iteration:\n",
        "\n",
        "Compute the negative gradient (residuals) of the loss function.\n",
        "\n",
        "Fit a weak learner to predict this gradient.\n",
        "\n",
        "Update the model:\n",
        "ğ¹\n",
        "ğ‘š\n",
        "(\n",
        "ğ‘¥\n",
        ")\n",
        "=\n",
        "ğ¹\n",
        "ğ‘š\n",
        "âˆ’\n",
        "1\n",
        "(\n",
        "ğ‘¥\n",
        ")\n",
        "+\n",
        "ğœ‚\n",
        "â‹…\n",
        "â„\n",
        "ğ‘š\n",
        "(\n",
        "ğ‘¥\n",
        ")\n",
        "F\n",
        "m\n",
        "â€‹\n",
        " (x)=F\n",
        "mâˆ’1\n",
        "â€‹\n",
        " (x)+Î·â‹…h\n",
        "m\n",
        "â€‹\n",
        " (x)\n",
        "where\n",
        "ğœ‚\n",
        "Î· is the learning rate,\n",
        "â„\n",
        "ğ‘š\n",
        "(\n",
        "ğ‘¥\n",
        ")\n",
        "h\n",
        "m\n",
        "â€‹\n",
        " (x) is the new tree.\n",
        "\n",
        "Repeat for a fixed number of iterations or until convergence.\n",
        "\n"
      ],
      "metadata": {
        "id": "eGcutOz_bIo6"
      }
    }
  ]
}