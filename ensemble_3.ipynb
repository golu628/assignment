{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNXHuh+wT1LiPBjO+OJ3Pe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/golu628/assignment/blob/main/ensemble_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdLEv255wQM1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Random Forest Regressor?\n",
        "\n",
        "Random Forest Regressor is a supervised learning algorithm that utilizes an ensemble technique called bagging to create a robust model for predicting continuous values (regression tasks). It combines the predictions from multiple decision trees to achieve higher accuracy and reduce the risk of overfitting.\n",
        "\n",
        "Q2. How does Random Forest Regressor reduce the risk of overfitting?\n",
        "\n",
        "Random Forest Regressor employs two key strategies to combat overfitting:\n",
        "\n",
        "Bootstrap Aggregation (Bagging): When building individual trees, the algorithm randomly samples subsets (with replacement) of the training data, creating a collection of diverse trees. This prevents any single tree from overfitting to specific patterns in the data.\n",
        "Random Feature Subsets: At each node of a decision tree, instead of considering all features, the algorithm randomly selects a subset for splitting. This injects some randomness into the tree-building process, further hindering overfitting to specific features.\n",
        "Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?\n",
        "\n",
        "There are two primary methods for aggregating predictions:\n",
        "\n",
        "Averaging: This is the most common approach. The final prediction is calculated as the average of the individual predictions from all decision trees in the forest.\n",
        "Voting: For classification problems (where you predict discrete categories), a voting mechanism can be used. The class with the most votes from the trees is chosen as the final prediction.\n",
        "Q4. What are the hyperparameters of Random Forest Regressor?\n",
        "\n",
        "Several hyperparameters control the behavior and performance of Random Forest Regressor. Some important ones include:\n",
        "\n",
        "n_estimators: The number of decision trees to create in the forest.\n",
        "max_depth: The maximum depth allowed for each tree (prevents overfitting by limiting tree complexity).\n",
        "min_samples_split: The minimum number of samples required to split a node (ensures sufficient data for splitting decisions).\n",
        "min_samples_leaf: The minimum number of samples allowed in a leaf node (prevents creation of very sparse trees).\n",
        "max_features: The number of features randomly considered at each split (introduces diversity among trees).\n",
        "Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?\n",
        "\n",
        "Structure: Random Forest Regressor is an ensemble model, combining multiple decision trees. Decision Tree Regressor is a single decision tree.\n",
        "Overfitting: Random Forest Regressor reduces overfitting through bagging and random feature subsets. Decision Tree Regressors are more susceptible to overfitting.\n",
        "Accuracy: Random Forest Regressors often have higher accuracy due to ensemble learning.\n",
        "Interpretability: Decision Tree Regressors are inherently more interpretable due to their single tree structure. Random Forests are less interpretable but can be explored using feature importance techniques.\n",
        "Q6. What are the advantages and disadvantages of Random Forest Regressor?\n",
        "\n",
        "Advantages:\n",
        "\n",
        "High accuracy and robustness\n",
        "Reduced overfitting risk\n",
        "Handles missing data inherently (by averaging over trees)\n",
        "Can provide feature importance\n",
        "Disadvantages:\n",
        "\n",
        "Less interpretable compared to decision trees\n",
        "Can be computationally expensive for large datasets\n",
        "Hyperparameter tuning can be complex\n",
        "Q7. What is the output of Random Forest Regressor?\n",
        "\n",
        "The output of a Random Forest Regressor is a predicted continuous value for each data point in the unseen (test) set. For instance, it might predict house prices, stock prices, or customer churn probability.\n",
        "\n",
        "Q8. Can Random Forest Regressor be used for classification tasks?\n",
        "\n",
        "No, Random Forest Regressor itself is specifically designed for regression problems. However, there's a variant called Random Forest Classifier that's adept at handling classification tasks. It utilizes the same ensemble approach with decision trees, but the final prediction is typically chosen through voting instead of averaging.\n",
        "\n",
        "pen_spark\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tune\n",
        "\n",
        "share\n",
        "\n",
        "\n",
        "more_vert\n"
      ],
      "metadata": {
        "id": "G2Q9CBlvwRB7"
      }
    }
  ]
}