{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvprFC5s9NQ9p9iCpFgD5Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/golu628/assignment/blob/main/ensemble_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dffnd3llntO7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rVHdCG4EpSkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is an ensemble technique in machine learning?\n",
        "\n",
        "Ensemble techniques combine multiple machine learning models to improve overall predictive performance. Instead of relying on a single model, they leverage the strengths of diverse models to achieve better accuracy and robustness.\n",
        "\n",
        "Q2. Why are ensemble techniques used in machine learning?\n",
        "\n",
        "There are several reasons to use ensemble techniques:\n",
        "\n",
        "Improved Accuracy: By combining multiple models, ensemble methods can often outperform individual models. Each model might learn slightly different patterns in the data, and the ensemble leverages this diversity.\n",
        "Reduced Variance: Some models are prone to high variance, meaning small changes in the training data can lead to significant changes in predictions. Ensembles can average out these variations and lead to more stable predictions.\n",
        "Reduced Bias: Some models might have a bias towards certain types of errors. By combining models with different biases, ensembles can mitigate this issue.\n",
        "Q3. What is bagging?\n",
        "\n",
        "Bagging (Bootstrap Aggregation) is a popular ensemble technique. Here's how it works:\n",
        "\n",
        "Create multiple training sets by randomly sampling (with replacement) from the original data. Each training set will have the same size as the original data, but some data points might be included multiple times, while others might be left out.\n",
        "Train a separate model on each of these bootstrapped training sets.\n",
        "For prediction, combine the predictions from all the individual models (e.g., by averaging their outputs).\n",
        "Q4. What is boosting?\n",
        "\n",
        "Boosting is another ensemble technique that works in a sequential manner:\n",
        "\n",
        "Train a weak learner (a simple model) on the entire dataset.\n",
        "Analyze the errors made by this model and create a new model focused on improving predictions for the data points the first model got wrong.\n",
        "Repeat steps 1 and 2, with each new model focusing on the errors of the previous one.\n",
        "Combine the predictions from all the models in a weighted fashion, where models with better performance on the training data get higher weights.\n",
        "Q5. What are the benefits of using ensemble techniques?\n",
        "\n",
        "Here are some key benefits:\n",
        "\n",
        "Improved accuracy and generalization\n",
        "Reduced variance and bias\n",
        "Can handle complex data patterns by leveraging multiple models\n",
        "Q6. Are ensemble techniques always better than individual models?\n",
        "\n",
        "Not necessarily. Ensemble techniques can be more complex to train and interpret compared to single models. Additionally, they might not always provide a significant improvement over a well-tuned individual model.\n",
        "\n",
        "Q7. How is the confidence interval calculated using bootstrap?\n",
        "\n",
        "Bootstrapping allows you to estimate the sampling distribution of a statistic (e.g., mean) by creating many resampled datasets (with replacement) from the original data. You can then calculate the statistic of interest (e.g., mean) on each resampled dataset. The confidence interval is then constructed from the distribution of these calculated statistics.\n",
        "\n",
        "Q8. How does bootstrap work and What are the steps involved in bootstrap?\n",
        "\n",
        "Here's the process of bootstrapping:\n",
        "\n",
        "Sample with replacement: Draw samples of the same size as the original data from your original data, allowing data points to be chosen multiple times. This creates a resampled dataset (bootstrap replicate).\n",
        "Repeat: Repeat step 1 multiple times (e.g., 1000 times) to create many bootstrap replicates.\n",
        "Analyze: Calculate the statistic of interest (e.g., mean) on each bootstrap replicate. This gives you a distribution of the statistic based on resampling.\n",
        "Confidence Interval: Use the percentiles of this distribution to construct the confidence interval for the population statistic.\n",
        "Q9. Confidence Interval for Mean Height\n",
        "\n",
        "Here's how to estimate the 95% confidence interval for the population mean height using bootstrap with your provided data:\n",
        "\n",
        "Sample with replacement: Draw 1000 bootstrap replicates by sampling 50 trees (with replacement) from your original data of 50 tree heights.\n",
        "Calculate Mean: For each replicate, calculate the mean height of the 50 trees. This gives you 1000 bootstrap means.\n",
        "Confidence Interval: Find the 2.5th and 97.5th percentiles of these 1000 bootstrap means. These values represent the lower and upper bounds of your 95% confidence interval.\n",
        "For example, if the 2.5th percentile is 13 meters and the 97.5th percentile is 17 meters, your 95% confidence interval for the population mean height would be (13 meters, 17 meters). This suggests that you're 95% confident that the true mean height of the population falls within this rang"
      ],
      "metadata": {
        "id": "atg3bVSjpSqx"
      }
    }
  ]
}