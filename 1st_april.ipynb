{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3gUPgn+ODdKQHd8o766pf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/golu628/assignment/blob/main/1st_april.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22MTVpUsTkbz"
      },
      "outputs": [],
      "source": [
        "Q1. Difference between Linear Regression and Logistic Regression\n",
        "\n",
        "Feature\tLinear Regression\tLogistic Regression\n",
        "Dependent Variable\tContinuous\tCategorical (binary)\n",
        "Output\tPredicted value\tProbability of an event (0 or 1)\n",
        "Model\tStraight line\tS-shaped curve (sigmoid function)\n",
        "Applications\tPredicting continuous values (e.g., house prices)\tClassification tasks (e.g., spam detection, credit risk assessment)\n",
        "\n",
        "drive_spreadsheet\n",
        "Export to Sheets\n",
        "Scenario favoring Logistic Regression:\n",
        "\n",
        "Imagine you want to predict whether an email is spam (1) or not spam (0). Linear regression wouldn't be suitable because it outputs continuous values, not probabilities. Logistic regression can model the probability of an email being spam based on features like sender address, keywords, etc.\n",
        "\n",
        "Q2. Cost Function and Optimization in Logistic Regression\n",
        "\n",
        "Cost Function: Logistic regression employs the binary cross-entropy loss function, which measures the difference between predicted probabilities and actual class labels. Lower loss signifies better model fit.\n",
        "Optimization:\n",
        "\n",
        "Common optimization algorithms like gradient descent or L-BFGS are used to minimize the cost function iteratively. These algorithms adjust model coefficients to reduce the loss and improve predictions.\n",
        "Q3. Regularization for Overfitting Prevention\n",
        "\n",
        "Regularization penalizes models for having overly large coefficients, preventing them from becoming too complex and overfitting to the training data. This helps the model generalize better to unseen data.\n",
        "\n",
        "L1 Regularization (Lasso): Shrinks some coefficients to zero, leading to feature selection (coefficients of zero-ed out features are not considered).\n",
        "L2 Regularization (Ridge): Shrinks all coefficients towards zero, reducing their magnitudes without complete elimination.\n",
        "Q4. ROC Curve for Model Evaluation\n",
        "\n",
        "The Receiver Operating Characteristic (ROC) curve is a visualization tool that measures the performance of a classification model. It plots the True Positive Rate (TPR) (correctly classified positives) against the False Positive Rate (FPR) (incorrectly classified negatives) for various classification thresholds.\n",
        "\n",
        "A high AUC (Area Under the Curve) on the ROC curve indicates good model performance at distinguishing between classes.\n",
        "Q5. Feature Selection Techniques\n",
        "\n",
        "Filter Methods: Rank features based on statistical measures like correlation with the target variable or information gain (mutual information). Select features with the highest scores.\n",
        "Wrapper Methods: Evaluate different feature subsets using the model's performance metric (e.g., accuracy). Choose the subset that yields the best performance.\n",
        "Embedded Methods: Feature selection is integrated into the model's training process. Examples include L1 regularization (Lasso) and decision trees.\n",
        "Benefits of Feature Selection:\n",
        "\n",
        "Improved model performance: Reduces irrelevant or redundant features, leading to a more focused model.\n",
        "Reduced training time: Fewer features translate to faster training.\n",
        "Increased interpretability: Easier to understand the impact of important features on the model.\n",
        "Q6. Handling Imbalanced Datasets\n",
        "\n",
        "Class imbalance occurs when one class (e.g., positive class) has significantly fewer samples than the other (negative class). This can lead to models biased towards the majority class.\n",
        "\n",
        "Strategies for Class Imbalance:\n",
        "\n",
        "Oversampling: Replicate samples from the minority class to create a more balanced dataset. Be cautious of overfitting.\n",
        "Undersampling: Randomly remove samples from the majority class to match the size of the minority class. May lose valuable data.\n",
        "Cost-Sensitive Learning: Assign higher weights to misclassified minority class samples during training, forcing the model to pay more attention to them.\n",
        "SMOTE (Synthetic Minority Over-sampling Technique): Creates synthetic samples for the minority class based on existing data.\n",
        "Q7. Common Issues and Challenges\n",
        "\n",
        "Multicollinearity: When independent variables are highly correlated, it can lead to unstable coefficient estimates and difficulty interpreting the model.\n",
        "Solution: Feature selection techniques can be used to identify and potentially remove redundant features.\n",
        "High dimensionality: Too many features can make it harder to train the model effectively.\n",
        "Solution: Feature selection and dimensionality reduction techniques can be used to reduce the number of features.\n",
        "Data quality: Poor data quality (e.g., missing values, outliers) can affect model performance.\n",
        "Solution: Data cleaning techniques like imputation and outlier detection can be applied.\n",
        "By carefully addressing these issues, you can enhance the robustness and"
      ]
    }
  ]
}