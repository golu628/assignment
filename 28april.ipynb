{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgOPdgMELx11V2wzrnKkGB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/golu628/assignment/blob/main/28april.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZT0IWLSnMaD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Unveiling Hierarchical Clustering and its Distinction:\n",
        "\n",
        "Hierarchical clustering is an unsupervised machine learning technique that unveils a hierarchy of clusters within your data. It doesn't require pre-defining the number of clusters, unlike K-Means clustering. It progressively builds a hierarchy of clusters by merging or splitting data points based on their similarity.\n",
        "\n",
        "Q2. Two Main Types of Hierarchical Clustering Algorithms:\n",
        "\n",
        "There are two main approaches to hierarchical clustering:\n",
        "\n",
        "Agglomerative (Bottom-Up): This is a more popular approach. It starts with each data point as an individual cluster and iteratively merges the most similar clusters until all data points belong to a single cluster.\n",
        "Divisive (Top-Down): This approach starts with all data points in one big cluster and then recursively splits the most dissimilar clusters until each cluster contains a single data point.\n",
        "Q3. Measuring the Distance Between Clusters:\n",
        "\n",
        "Hierarchical clustering relies on a distance metric to determine how close clusters are. Common distance metrics include:\n",
        "\n",
        "Euclidean distance: Measures the straight-line distance between data points.\n",
        "Manhattan distance: Measures the total distance traveled along each axis to get from one point to another (like taxicab geometry).\n",
        "Jaccard distance: Used for categorical data, it measures the ratio of mismatches between sets of data points.\n",
        "Q4. Finding the Optimal Number of Clusters (A Challenge):\n",
        "\n",
        "There's no single perfect way to determine the optimal number of clusters in hierarchical clustering. Here are common methods:\n",
        "\n",
        "Dendrogram analysis: Visually inspecting the dendrogram (a tree-like structure showing cluster mergers) to identify a level where the distance between merged clusters sharply increases.\n",
        "Silhouette analysis: Measures how well each data point fits its assigned cluster compared to neighboring clusters.\n",
        "Gap statistic: Compares within-cluster distances to an expected distribution of distances under a null hypothesis of random clustering.\n",
        "Q5. Dendrograms: Visualizing the Cluster Hierarchy:\n",
        "\n",
        "Dendrograms are tree-like diagrams that depict the merging process of clusters in hierarchical clustering. The horizontal axis represents the distance between merged clusters, and the vertical axis shows the hierarchy. Analyzing dendrograms helps identify the appropriate number of clusters by looking for significant jumps in distance between merges.\n",
        "\n",
        "Q6. Numerical vs. Categorical Data and Distance Metrics:\n",
        "\n",
        "Hierarchical clustering can handle both numerical and categorical data. However, distance metrics differ:\n",
        "\n",
        "Numerical data: Euclidean and Manhattan distances are commonly used.\n",
        "Categorical data: Jaccard distance or other set similarity measures are preferred.\n",
        "Q7. Identifying Outliers with Hierarchical Clustering:\n",
        "\n",
        "While hierarchical clustering doesn't explicitly identify outliers, it can provide clues. Data points that consistently remain far away from other points during merging might be potential outliers. Analyzing the structure of the dendrogram can also reveal isolated branches that might indicate outliers.\n",
        "\n",
        "pen_spark\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tune\n",
        "\n",
        "share\n",
        "\n",
        "\n",
        "more_vert\n"
      ],
      "metadata": {
        "id": "hGNErxo_nQje"
      }
    }
  ]
}